{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark\n",
    "from pyspark import SparkContext\n",
    "\n",
    "sc =SparkContext.getOrCreate()\n",
    "from pyspark.sql import SQLContext\n",
    "trurl='/home/aaftab/Dropbox/Techsoc/train_techsoc.csv'\n",
    "tsturl='/home/aaftab/Dropbox/Techsoc/test_techsoc.csv'\n",
    "\n",
    "sc.addFile(trurl)\n",
    "sc.addFile(tsturl)\n",
    "\n",
    "sqlcontext=SQLContext(sc)\n",
    "\n",
    "from pyspark import SparkFiles\n",
    "trdf=sqlcontext.read.csv(SparkFiles.get('train_techsoc.csv'), header=True, inferSchema=True)\n",
    "tstdf=sqlcontext.read.csv(SparkFiles.get('test_techsoc.csv'), header=True, inferSchema=True)\n",
    "\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "vectorAssembler = VectorAssembler(inputCols = ['x_sim','y_sim','z_sim','Vx_sim','Vy_sim','Vz_sim','sat_id','x_sim_ff','y_sim_ff','z_sim_ff','Vx_sim_ff','Vy_sim_ff','Vz_sim_ff','x_sim_bf','y_sim_bf','z_sim_bf','Vx_sim_bf','Vy_sim_bf','Vz_sim_bf'], outputCol = 'features')\n",
    "trd=vectorAssembler.transform(trdf)\n",
    "\n",
    "trdx=trd.select(['features','x'])\n",
    "trdy=trd.select(['features','y'])\n",
    "trdz=trd.select(['features','z'])\n",
    "\n",
    "trdVx=trd.select(['features','Vx'])\n",
    "trdVy=trd.select(['features','Vy'])\n",
    "trdVz=trd.select(['features','Vz'])\n",
    "\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "vectorAssembler = VectorAssembler(inputCols =['x_sim','y_sim','z_sim','Vx_sim','Vy_sim','Vz_sim','sat_id','x_sim_ff','y_sim_ff','z_sim_ff','Vx_sim_ff','Vy_sim_ff','Vz_sim_ff','x_sim_bf','y_sim_bf','z_sim_bf','Vx_sim_bf','Vy_sim_bf','Vz_sim_bf'], outputCol = 'features')\n",
    "tstd=vectorAssembler.transform(tstdf)\n",
    "tstd=tstd.select(['features'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import MinMaxScaler\n",
    "# Let us create an object of MinMaxScaler class\n",
    "MinMaxScalerizer=MinMaxScaler().setMin(5).setMax(10).setInputCol(\"features\").setOutputCol(\"minmax_features\")\n",
    "\n",
    "transx=MinMaxScalerizer.fit(trdx).transform(trdx)\n",
    "transy=MinMaxScalerizer.fit(trdy).transform(trdy)\n",
    "transz=MinMaxScalerizer.fit(trdy).transform(trdz)\n",
    "\n",
    "transVx=MinMaxScalerizer.fit(trdVx).transform(trdVx)\n",
    "transVy=MinMaxScalerizer.fit(trdVy).transform(trdVy)\n",
    "transVz=MinMaxScalerizer.fit(trdVz).transform(trdVz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|            features|     minmax_features|\n",
      "+--------------------+--------------------+\n",
      "|[-67817.397487337...|[7.48023212002414...|\n",
      "|[-69478.899261576...|[7.47236845339599...|\n",
      "|[-69610.589087625...|[7.47174518298700...|\n",
      "|[-68136.264984711...|[7.47872296262402...|\n",
      "|[-64959.510617139...|[7.49375811798372...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import MinMaxScaler\n",
    "# Let us create an object of MinMaxScaler class\n",
    "MinMaxScalerizer=MinMaxScaler().setMin(5).setMax(10).setInputCol(\"features\").setOutputCol(\"minmax_features\")\n",
    "transt=MinMaxScalerizer.fit(trd).transform(tstd)\n",
    "transt.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import PCA\n",
    "pca = PCA(k=8, inputCol=\"minmax_features\", outputCol=\"pcaFeatures\")\n",
    "\n",
    "modelx = pca.fit(transx)\n",
    "transformedx=modelx.transform(transx)\n",
    "modely = pca.fit(transy)\n",
    "transformedy=modely.transform(transy)\n",
    "modelz = pca.fit(transz)\n",
    "transformedz=modelz.transform(transz)\n",
    "\n",
    "modelVx = pca.fit(transVx)\n",
    "transformedVx=modelVx.transform(transVx)\n",
    "modelVy = pca.fit(transVy)\n",
    "transformedVy=modelVy.transform(transVy)\n",
    "modelVz = pca.fit(transVz)\n",
    "transformedVz=modelVz.transform(transVz)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9815558570645541\n",
      "+--------------------+--------------------+--------------------+\n",
      "|            features|     minmax_features|         pcaFeatures|\n",
      "+--------------------+--------------------+--------------------+\n",
      "|[-67817.397487337...|[7.48023212002414...|[-5.5218024783223...|\n",
      "|[-69478.899261576...|[7.47236845339599...|[-5.5225038026794...|\n",
      "|[-69610.589087625...|[7.47174518298700...|[-5.5237439867107...|\n",
      "|[-68136.264984711...|[7.47872296262402...|[-5.5252047030163...|\n",
      "|[-64959.510617139...|[7.49375811798372...|[-5.5269114341109...|\n",
      "+--------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import PCA\n",
    "pca = PCA(k=8, inputCol=\"minmax_features\", outputCol=\"pcaFeatures\")\n",
    "modelt = pca.fit(transt)\n",
    "print(sum(modelt.explainedVariance))\n",
    "transformedt=modelt.transform(transt)\n",
    "transformedt.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "rfx=LinearRegression(featuresCol = 'minmax_features', labelCol = 'x')\n",
    "rfy=LinearRegression(featuresCol = 'minmax_features', labelCol = 'y')\n",
    "rfz=LinearRegression(featuresCol = 'minmax_features', labelCol = 'z')\n",
    "\n",
    "rfVx=LinearRegression(featuresCol = 'minmax_features', labelCol = 'Vx')\n",
    "rfVy=LinearRegression(featuresCol = 'minmax_features', labelCol = 'Vy')\n",
    "rfVz=LinearRegression(featuresCol = 'minmax_features', labelCol = 'Vz')\n",
    "\n",
    "rfmodelx = rfx.fit(transformedx)\n",
    "rfmodely = rfy.fit(transformedy)\n",
    "rfmodelz = rfz.fit(transformedz)\n",
    "\n",
    "rfmodelVx = rfVx.fit(transformedVx)\n",
    "rfmodelVy = rfVy.fit(transformedVy)\n",
    "rfmodelVz = rfVz.fit(transformedVz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "rfpredx=rfmodelx.transform(transformedt)\n",
    "rfpredx.select(['prediction']).toPandas().to_csv('x.csv')\n",
    "rfpredy=rfmodely.transform(transformedt)\n",
    "rfpredy.select(['prediction']).toPandas().to_csv('y.csv')\n",
    "rfpredz=rfmodelz.transform(transformedt)\n",
    "rfpredz.select(['prediction']).toPandas().to_csv('z.csv')\n",
    "\n",
    "rfpredVx=rfmodelVx.transform(transformedt)\n",
    "rfpredVx.select(['prediction']).toPandas().to_csv('Vx.csv')\n",
    "rfpredVy=rfmodelVy.transform(transformedt)\n",
    "rfpredVy.select(['prediction']).toPandas().to_csv('Vy.csv')\n",
    "rfpredVz=rfmodelVz.transform(transformedt)\n",
    "rfpredVz.select(['prediction']).toPandas().to_csv('Vz.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "x = pd.read_csv('/home/aaftab/spark/spark-3.0.0-preview2-bin-hadoop2.7/x.csv')\n",
    "y = pd.read_csv('/home/aaftab/spark/spark-3.0.0-preview2-bin-hadoop2.7/y.csv')\n",
    "z= pd.read_csv('/home/aaftab/spark/spark-3.0.0-preview2-bin-hadoop2.7/z.csv')\n",
    "\n",
    "Vx=pd.read_csv('/home/aaftab/spark/spark-3.0.0-preview2-bin-hadoop2.7/Vx.csv')\n",
    "Vy=pd.read_csv('/home/aaftab/spark/spark-3.0.0-preview2-bin-hadoop2.7/Vy.csv')\n",
    "Vz=pd.read_csv('/home/aaftab/spark/spark-3.0.0-preview2-bin-hadoop2.7/Vz.csv')\n",
    "\n",
    "df3 = pd.concat([x,y,z,Vx,Vy,Vz], axis=1)\n",
    "df3.to_csv('file3.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
